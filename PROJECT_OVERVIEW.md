# Fine-Tuning & Deployment Pipeline

Complete end-to-end solution for training, quantizing, and deploying custom AI models.

## ğŸ“ Project Structure

```
finetuningscript/
â”‚
â”œâ”€â”€ unsloth/                        # Training environment
â”‚   â”œâ”€â”€ train.py                    # Main training script
â”‚   â”œâ”€â”€ create_balanced_dataset.py  # Dataset preparation
â”‚   â”œâ”€â”€ data/                       # Training data (JSONL files)
â”‚   â”œâ”€â”€ export_and_quantize.py      # Model export script
â”‚   â”œâ”€â”€ merge_lora.py               # LoRA merging
â”‚   â””â”€â”€ POST_TRAINING.md            # Post-training guide
â”‚
â”œâ”€â”€ finetunedmodels/                # Exported models
â”‚   â””â”€â”€ qwen3-1.7b-trading-q4km/    # Your fine-tuned model
â”‚       â”œâ”€â”€ qwen3-1.7b-trading-Q4_K_M.gguf  # Quantized model
â”‚       â”œâ”€â”€ tokenizer files
â”‚       â””â”€â”€ README.md
â”‚
â””â”€â”€ serverhost/                     # Deployment setup
    â”œâ”€â”€ docker-compose.yml          # Service orchestration
    â”œâ”€â”€ setup.sh                    # Automated setup
    â”œâ”€â”€ models/                     # Model storage
    â”œâ”€â”€ upload-ui/                  # Model management UI
    â”œâ”€â”€ README.md                   # Deployment guide
    â””â”€â”€ QUICKSTART.md               # Quick start
```

---

## ğŸ¯ Three Main Components

### 1. Training (`unsloth/`)

**Purpose**: Fine-tune language models efficiently

**Key Features**:
- Unsloth for fast training
- LoRA for parameter-efficient fine-tuning
- Balanced dataset (trading + tools + reasoning)
- WandB integration for tracking

**Quick Start**:
```bash
cd unsloth
bash setup_vast.sh              # On training server
python create_balanced_dataset.py
python train.py
```

**Output**: Fine-tuned LoRA adapters

---

### 2. Export & Quantization (`finetunedmodels/`)

**Purpose**: Convert and optimize models for deployment

**Process**:
1. Merge LoRA with base model (3.46 GB)
2. Convert to GGUF format
3. Quantize to Q4_K_M (1.05 GB)

**Scripts**:
```bash
python merge_lora.py                    # Merge adapters
python export_and_quantize.py           # Export & prepare
bash quantize_to_gguf.sh                # Quantize
bash download_model.sh                  # Download
```

**Output**: Production-ready GGUF model

---

### 3. Deployment (`serverhost/`)

**Purpose**: Host the model with web interfaces

**Components**:
- **llama.cpp server** (Port 8080) - AI chat interface
- **Upload UI** (Port 3000) - Model management

**Quick Start**:
```bash
cd serverhost
bash setup.sh
# Open http://localhost:3000 and http://localhost:8080
```

**Configured For**:
- CPU-only inference (6 cores, 8GB RAM)
- Docker-based deployment
- Multiple model support

---

## ğŸ”„ Complete Workflow

### Step 1: Train Your Model

```bash
# On training server (e.g., vast.ai with A100)
cd unsloth
python create_balanced_dataset.py
python train.py
```

**Duration**: ~48 minutes (3 epochs)  
**Output**: `output_model/` with LoRA adapters

### Step 2: Export & Quantize

```bash
# On training server
python merge_lora.py                           # â†’ output_model_merged/ (3.46 GB)
python llama.cpp/convert_hf_to_gguf.py ...     # â†’ FP16 GGUF (3.44 GB)
./llama.cpp/build/bin/llama-quantize ...       # â†’ Q4_K_M GGUF (1.05 GB)
```

**Output**: `qwen3-1.7b-trading-Q4_K_M.gguf`

### Step 3: Download Model

```bash
# From local machine
rsync -avz server:/path/to/model.gguf finetunedmodels/
```

### Step 4: Deploy

```bash
# On deployment server (or local)
cd serverhost
bash setup.sh

# Access:
# - Upload UI: http://localhost:3000
# - Chat UI: http://localhost:8080
```

---

## ğŸ® Usage Examples

### Training a New Model

```bash
# 1. Prepare data
cd unsloth
python create_balanced_dataset.py

# 2. Train
python train.py

# 3. Export
python merge_lora.py
python export_and_quantize.py

# 4. Quantize
bash quantize_to_gguf.sh

# 5. Download
bash download_model.sh
```

### Deploying a Model

```bash
# 1. Copy model to serverhost
cp finetunedmodels/your-model/*.gguf serverhost/models/

# 2. Setup and run
cd serverhost
bash setup.sh

# 3. Open browser
open http://localhost:3000  # Management
open http://localhost:8080  # Chat
```

### Switching Models

```bash
# Option 1: Via Upload UI
# - Visit http://localhost:3000
# - Click "Activate" on desired model
# - Restart: docker compose restart llama-server

# Option 2: Via CLI
cd serverhost/models
ln -sf new-model.gguf current.gguf
docker compose restart llama-server
```

---

## ğŸ“Š Resource Requirements

### Training (Recommended)

- **GPU**: NVIDIA A100 40GB (or similar)
- **RAM**: 32GB+
- **Storage**: 50GB
- **Time**: ~1 hour per training run

### Deployment (Your Setup)

- **CPU**: 6 cores
- **RAM**: 8GB
- **Storage**: ~2GB per model
- **Performance**: 2-8 tokens/sec

---

## ğŸ”§ Configuration Files

### Key Files to Customize

**Training**:
- `unsloth/train.py` - Training hyperparameters
- `unsloth/.env` - API keys (HF_TOKEN, WANDB_API_KEY)
- `unsloth/data/` - Your training data

**Deployment**:
- `serverhost/docker-compose.yml` - Server settings
- CPU threads, memory limits, context size
- Port mappings

---

## ğŸ¯ Typical Use Cases

### 1. Domain-Specific Assistant

Train on your domain data â†’ Deploy for internal use

```
Your Data â†’ Training â†’ Export â†’ Deploy â†’ Team Access
```

### 2. Multiple Versions

Train different versions â†’ Switch via Upload UI

```
Model v1.0 â”€â”€â”
Model v1.1 â”€â”€â”¼â”€â”€â†’ Upload UI â†’ Activate â†’ Use
Model v2.0 â”€â”€â”˜
```

### 3. Continuous Improvement

Retrain with new data â†’ Update deployment

```
New Data â†’ Retrain â†’ Export â†’ Upload â†’ Switch
```

---

## ğŸ“š Documentation Index

| Component | Guide | Description |
|-----------|-------|-------------|
| Training | `unsloth/README.md` | Setup & training |
| Post-Training | `unsloth/POST_TRAINING.md` | Export & quantize |
| Model Info | `finetunedmodels/.../README.md` | Model details |
| Deployment | `serverhost/README.md` | Full deployment guide |
| Quick Start | `serverhost/QUICKSTART.md` | 3-minute setup |

---

## ğŸ†˜ Quick Help

### Training Issues

```bash
cd unsloth
# Check environment
python check_env.py

# Fix PyTorch
bash fix_torchvision.sh

# View logs
tail -f training_balanced.log
```

### Deployment Issues

```bash
cd serverhost
# Check services
docker compose ps

# View logs
docker compose logs -f

# Restart
docker compose restart
```

### Model Issues

```bash
# Verify GGUF file
file models/current.gguf

# Test loading
./llama.cpp/build/bin/llama-cli -m models/current.gguf -p "test"

# Check size
ls -lh models/*.gguf
```

---

## ğŸš€ Performance Tips

### Training

- Use Flash Attention 2 (enabled by default)
- Enable packing for better GPU utilization
- Balance dataset to prevent catastrophic forgetting
- Monitor with WandB

### Inference (CPU)

- Use Q4_K_M quantization (best balance)
- Limit context to 2048-4096 tokens
- Single concurrent user
- Monitor with `docker stats`

---

## ğŸ”’ Security Considerations

### Training Server

- Use environment variables for tokens
- Don't commit `.env` files
- Secure vast.ai/training instance

### Deployment Server

- Currently: No authentication (localhost only)
- Production: Add reverse proxy + auth
- Use firewall rules
- Enable HTTPS for external access

---

## ğŸ“ˆ Monitoring

### Training

```bash
# WandB dashboard
https://wandb.ai/your-project

# Local logs
tail -f unsloth/training_balanced.log
```

### Deployment

```bash
# Container stats
docker stats

# Server logs
docker compose logs -f llama-server

# API health
curl http://localhost:8080/health
```

---

## ğŸ‰ Success Checklist

- [ ] **Trained** model on GPU server
- [ ] **Exported** to GGUF format
- [ ] **Quantized** to Q4_K_M
- [ ] **Downloaded** to local machine
- [ ] **Deployed** with Docker
- [ ] **Accessed** both UIs (3000 & 8080)
- [ ] **Tested** chat functionality
- [ ] **Verified** tool-calling works

---

## ğŸ“ Support Resources

- **llama.cpp**: https://github.com/ggml-org/llama.cpp
- **Unsloth**: https://github.com/unslothai/unsloth
- **WandB**: https://wandb.ai
- **Docker**: https://docs.docker.com

---

## ğŸ“ Learning Path

1. **Start**: Run `serverhost/setup.sh` â†’ Try the chat
2. **Explore**: Upload different models via UI
3. **Customize**: Adjust settings in docker-compose.yml
4. **Train**: Create your own dataset â†’ Fine-tune
5. **Scale**: Deploy on production server

---

**Made with â¤ï¸ for the AI community**

Questions? Check the README files in each directory!

